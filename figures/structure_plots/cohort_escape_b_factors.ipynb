{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38aa54f7-e9c3-49f0-93df-4c2c551c73a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import altair as alt\n",
    "import altair_saver\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import polyclonal\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import Bio\n",
    "\n",
    "from IPython.utils import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c14f9084-0391-4e71-b4e1-e5fa823d139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be69c410-a39f-4d21-b0c0-8b8dc7691e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summed_escapes(sera_list, age_group, site_list=None):\n",
    "    summed_escape_list = []\n",
    "    \n",
    "    for serum in sera_list:\n",
    "        prob_escape = pd.read_csv(\n",
    "            f'results/antibody_escape/{serum}_avg.csv'\n",
    "        ).query(\n",
    "            \"`times_seen` >= 5\"\n",
    "        )\n",
    "        \n",
    "        prob_escape_sum = (\n",
    "            prob_escape.groupby(['site', 'wildtype'], as_index=False)\n",
    "            .aggregate({'escape_mean': 'sum'})\n",
    "            .rename(columns={'escape_mean': 'escape'})\n",
    "        )\n",
    "\n",
    "        if site_list:\n",
    "            prob_escape_final = prob_escape_sum[prob_escape_sum['site'].isin(site_list)]\n",
    "            prob_escape_final['site'] = pd.Categorical(prob_escape_final['site'], ordered=True)\n",
    "            # prob_escape_final['site'] = prob_escape_final['site'].astype(str)\n",
    "\n",
    "        else:\n",
    "            prob_escape_final = prob_escape_sum.copy()\n",
    "            \n",
    "        prob_escape_final['serum'] = serum\n",
    "        prob_escape_final['cohort'] = age_group\n",
    "        \n",
    "        summed_escape_list.append(prob_escape_final)\n",
    "        \n",
    "    summed_escape = pd.concat(summed_escape_list)\n",
    "    return summed_escape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57b77088-df77-414e-8c10-483e5b195210",
   "metadata": {},
   "outputs": [],
   "source": [
    "peds = [3944, 2389, 2323, 2388, 3973, 4299, 4584, 2367]\n",
    "teens = [2350, 2365, 2382, 3866, 2380, 3856, 3857, 3862]\n",
    "adults = ['33C', '34C', '197C', '199C', '215C', \n",
    "          '210C', '74C', '68C', '150C', '18C',]\n",
    "\n",
    "sample_lists = [peds, teens, adults]\n",
    "cohorts = ['2-5_years', '15-20_years', '40-45_years']\n",
    "\n",
    "summed_escapes = []\n",
    "i=0 # for looping through age cohort definitions\n",
    "\n",
    "for entry in sample_lists:\n",
    "    summed_escape = get_summed_escapes(entry, cohorts[i])\n",
    "    summed_escapes.append(summed_escape)\n",
    "\n",
    "    i+=1\n",
    "\n",
    "escape_df_hk19 = pd.concat(summed_escapes)\n",
    "\n",
    "escape_df_hk19['serum'] = escape_df_hk19['serum'].astype(str)\n",
    "escape_df_hk19['ha_strain'] = 'hk19'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b028a0cf-6839-46c1-8c00-9b155baa54fd",
   "metadata": {},
   "source": [
    "### process Perth09 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9611eb7-3b20-4255-b7a8-2f27d8da470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define samples in each age cohort\n",
    "sample_dict = {\n",
    "    \"2-4_years\": [\n",
    "        \"age 2.1 (Vietnam)\", \n",
    "        \"age 2.2 (Vietnam)\",\n",
    "        \"age 2.4 (Vietnam)\",\n",
    "        \"age 2.5 (Vietnam)\",\n",
    "        \"age 2.5b (Vietnam)\",\n",
    "        \"age 3.3 (Vietnam)\", \n",
    "        \"age 3.3b (Vietnam)\",\n",
    "        \"age 3.4 (Vietnam)\", \n",
    "        \"age 3.5 (Vietnam)\",\n",
    "    ],   \n",
    "    \"30-34_years\": [\n",
    "        \"age 30.5 (Vietnam)\",\n",
    "        \"age 31.5 (Vietnam)\",\n",
    "        \"age 33.5 (Vietnam)\",\n",
    "    ],\n",
    "    \"misc_adult\": [\n",
    "        \"age 21 (Seattle)\",\n",
    "        \"age 53 (Seattle)\",\n",
    "        \"age 64 (Seattle)\",\n",
    "        \"age 65 (Seattle)\",\n",
    "    ],\n",
    "    \"ferret\": [\n",
    "        \"ferret 1 (Pitt)\",\n",
    "        \"ferret 2 (Pitt)\",\n",
    "        \"ferret 3 (Pitt)\",\n",
    "        \"ferret (WHO)\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "# get full dataset\n",
    "escape_df = pd.read_csv(f'results/perth2009/merged_escape.csv')[['name', 'site', 'wildtype', 'mutant', 'escape']]\n",
    "escape_df = escape_df.rename(columns={'name': 'serum'})\n",
    "\n",
    "# Function to convert '(HA2)X' to numeric\n",
    "def convert_site_to_numeric(site):\n",
    "    if '(HA2)' in site:\n",
    "        try:\n",
    "            number = int(site.replace('(HA2)', '').strip())\n",
    "            return number + 329\n",
    "        except ValueError:\n",
    "            return site  # If there's an issue with conversion, return the original value\n",
    "    else:\n",
    "        return site\n",
    "\n",
    "# Apply the function to the 'site' column\n",
    "escape_df['site'] = escape_df['site'].apply(convert_site_to_numeric)\n",
    "\n",
    "# get summed escape at each site\n",
    "escape_df = escape_df.groupby(['serum', 'site', 'wildtype'], as_index=False).aggregate({'escape': 'sum'})\n",
    "\n",
    "# floor at 0\n",
    "escape_df['escape'] = escape_df['escape'].clip(lower=0)\n",
    "\n",
    "# add cohort label\n",
    "def find_sample_type(sample_name):\n",
    "    for sample_type, sample_list in sample_dict.items():\n",
    "        if sample_name in sample_list:\n",
    "            return sample_type\n",
    "    return None\n",
    "\n",
    "escape_df['cohort'] = escape_df['serum'].apply(find_sample_type)\n",
    "\n",
    "escape_df = escape_df.loc[(escape_df['cohort'] != 'misc_adult') & (escape_df['cohort'] != 'ferret')]\n",
    "escape_df['site'] = escape_df['site'].astype(int)\n",
    "\n",
    "escape_df['ha_strain'] = 'perth09'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16158a5b-e5d9-42b3-b125-937080ac428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "escape_df_full = pd.concat([escape_df, escape_df_hk19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af9a9029-adf5-4f7f-9748-fda4c030e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the DataFrame by the 'serum' column\n",
    "grouped = escape_df_full.groupby('serum')\n",
    "\n",
    "# Define a function to normalize the 'escape_mean' column within each group\n",
    "def normalize(group):\n",
    "    group['escape'] = group['escape'] / group['escape'].max()\n",
    "    return group\n",
    "\n",
    "# Apply the normalization function to each group\n",
    "normalized_df = grouped.apply(normalize)\n",
    "\n",
    "# Reset the index of the resulting DataFrame\n",
    "normalized_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "normalized_df['escape'] = normalized_df['escape'].clip(lower=0)\n",
    "\n",
    "# replace mean_escape_mean values\n",
    "normalized_df['mean_cohort_escape'] = (\n",
    "    normalized_df.groupby(['site', 'cohort'])['escape']\n",
    "    .transform('mean')\n",
    ")\n",
    "\n",
    "# also add max value at each site within a cohort\n",
    "normalized_df['max_cohort_escape'] = (\n",
    "    normalized_df.groupby(['site', 'cohort'])['escape']\n",
    "    .transform('max')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "364bfd91-8429-4570-98a0-024b6cc94a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame 'normalized_cohort_df' by grouping 'normalized_df' by the 'cohort' column\n",
    "grouped = normalized_df.groupby('cohort')\n",
    "\n",
    "# Initialize an empty list to store DataFrames for each cohort\n",
    "cohort_dfs = []\n",
    "\n",
    "# Loop through each cohort group\n",
    "for cohort, cohort_data in grouped:\n",
    "    # Create a DataFrame containing unique sites and their mean_cohort_escape values\n",
    "    cohort_escape = cohort_data[['site', 'mean_cohort_escape', 'max_cohort_escape', 'ha_strain', 'cohort']].drop_duplicates()\n",
    "\n",
    "    # Add chain identifiers, which will duplicate sites 3x\n",
    "    chain_dfs = []\n",
    "    chains = ['A', 'C', 'E']\n",
    "    for chain in chains:\n",
    "        chain_df = cohort_escape.copy()\n",
    "        chain_df['chain'] = chain\n",
    "        # cohort_escape['chain'] = chain\n",
    "        chain_dfs.append(chain_df)\n",
    "\n",
    "    cohort_escape = pd.concat(chain_dfs, ignore_index=True)\n",
    "    \n",
    "    # Append this cohort's data to the list\n",
    "    cohort_dfs.append(cohort_escape)\n",
    "\n",
    "# Concatenate all the DataFrames in the list to create the final 'normalized_cohort_df'\n",
    "# normalized_cohort_df = pd.concat(cohort_dfs, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "904ce066-5878-49ad-ab9e-075e9675eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reassign_b_factor(\n",
    "    input_pdbfile,\n",
    "    output_pdbfile,\n",
    "    df,\n",
    "    metric_col,\n",
    "    *,\n",
    "    site_col=\"site\",\n",
    "    chain_col=\"chain\",\n",
    "    missing_metric=0,\n",
    "    model_index=0,\n",
    "):\n",
    "\n",
    "    # subset `df` to needed columns and error check it\n",
    "    cols = [metric_col, site_col, chain_col]\n",
    "    for col in cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"`df` lacks column {col}\")\n",
    "    df = df[cols].drop_duplicates()\n",
    "    if len(df) != len(df.groupby([site_col, chain_col])):\n",
    "        raise ValueError(\"non-unique metric for a site in a chain\")\n",
    "\n",
    "#     if df[site_col].dtype != int:\n",
    "#         raise ValueError(\"function currently requires `site_col` to be int\")\n",
    "\n",
    "    # read PDB, catch warnings about discontinuous chains\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\n",
    "            \"ignore\", category=Bio.PDB.PDBExceptions.PDBConstructionWarning\n",
    "        )\n",
    "        pdb = Bio.PDB.PDBParser().get_structure(\"_\", input_pdbfile)\n",
    "\n",
    "    # get the model out of the PDB\n",
    "    model = list(pdb.get_models())[model_index]\n",
    "\n",
    "    # make sure all chains in PDB\n",
    "    missing_chains = set(df[chain_col]) - {chain.id for chain in model.get_chains()}\n",
    "    if missing_chains:\n",
    "        raise ValueError(f\"`df` has chains not in PDB: {missing_chains}\")\n",
    "\n",
    "    # make missing_metric a dict if it isn't already\n",
    "    if not isinstance(missing_metric, dict):\n",
    "        missing_metric = {chain.id: missing_metric for chain in model.get_chains()}\n",
    "\n",
    "    # loop over all chains and do coloring\n",
    "    for chain in model.get_chains():\n",
    "        chain_id = chain.id\n",
    "        site_to_val = (\n",
    "            df.query(f\"{chain_col} == @chain_id\")\n",
    "            .set_index(site_col)[metric_col]\n",
    "            .to_dict()\n",
    "        )\n",
    "        for residue in chain:\n",
    "            site = residue.get_id()[1]\n",
    "            try:\n",
    "                metric_val = site_to_val[site]\n",
    "            except KeyError:\n",
    "                metric_val = missing_metric[chain_id]\n",
    "            # for disordered residues, get list of them\n",
    "            try:\n",
    "                residuelist = residue.disordered_get_list()\n",
    "            except AttributeError:\n",
    "                residuelist = [residue]\n",
    "            for r in residuelist:\n",
    "                for atom in r:\n",
    "                    # for disordered atoms, get list of them\n",
    "                    try:\n",
    "                        atomlist = atom.disordered_get_list()\n",
    "                    except AttributeError:\n",
    "                        atomlist = [atom]\n",
    "                    for a in atomlist:\n",
    "                        a.bfactor = metric_val\n",
    "\n",
    "    # write PDB\n",
    "    io = Bio.PDB.PDBIO()\n",
    "    io.set_structure(pdb)\n",
    "    io.save(output_pdbfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "403365bb-f94b-41f8-aa4c-f0c0382d24e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in cohort_dfs:\n",
    "    cohort_name = df['cohort'].iloc[0]\n",
    "    ha_strain = df['ha_strain'].iloc[0]\n",
    "    reassign_b_factor(input_pdbfile='data/PDBs/4o5n.pdb',\n",
    "                      output_pdbfile=f'scratch_notebooks/figure_drafts/structure_plots/{ha_strain}_{cohort_name}_normalized_max.pdb',\n",
    "                      df=df,\n",
    "                      metric_col='max_cohort_escape',\n",
    "                      site_col=\"site\",\n",
    "                      chain_col=\"chain\",\n",
    "                      missing_metric=0,\n",
    "                      model_index=0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df06f29-8208-445d-a1e2-0ed0b2a8cbc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
