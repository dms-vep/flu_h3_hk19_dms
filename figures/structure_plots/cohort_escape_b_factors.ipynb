{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d562103-3472-4f48-bec0-62ca1aa779f2",
   "metadata": {},
   "source": [
    "# Reassign B factors for visualizing escape on the H3 HA structure\n",
    "In order to visualize escape, we reassign the B-factors to escape scores for each site (i.e. residue), then color by B-factors in PyMol. This notebook generates PDB files with B-factors assigned to the maximum normalized escape score for each cohort analyzed in this paper. \n",
    "\n",
    "We chose to use the normalized escape scores because these values are more directly comparable between the Perth/2009 and HongKong/2019 data. We chose the maximum normalized escape scores because our goal was to visualize the differences in regions targeted by **any** serum in a cohort, rather than the average escape. Using the maximum accounts for the heterogeneity in targeting within certain cohorts (e.g. the average escape is relatively low because only one or two individuals have appreciable neutralization escape at that site). These structures are shown in Figure 5.\n",
    "\n",
    "These pdb files are output in the folder `escape_score_pdbs/`, and are used as inputs for the script `cohort_escape_viz.py`. Final recolored structures are saved in `figure_5/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38aa54f7-e9c3-49f0-93df-4c2c551c73a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import polyclonal\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import Bio\n",
    "\n",
    "from IPython.utils import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c14f9084-0391-4e71-b4e1-e5fa823d139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c08703d-467b-49b8-a002-65ffe73558d4",
   "metadata": {},
   "source": [
    "### Get HongKong/19 escape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be69c410-a39f-4d21-b0c0-8b8dc7691e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summed_escapes(sera_list, age_group, site_list=None):\n",
    "    summed_escape_list = []\n",
    "    \n",
    "    for serum in sera_list:\n",
    "        prob_escape = pd.read_csv(\n",
    "            f'results/antibody_escape/{serum}_avg.csv'\n",
    "        ).query(\n",
    "            \"`times_seen` >= 5\"\n",
    "        )\n",
    "        \n",
    "        prob_escape_sum = (\n",
    "            prob_escape.groupby(['site', 'wildtype'], as_index=False)\n",
    "            .aggregate({'escape_mean': 'sum'})\n",
    "            .rename(columns={'escape_mean': 'escape'})\n",
    "        )\n",
    "\n",
    "        if site_list:\n",
    "            prob_escape_final = prob_escape_sum[prob_escape_sum['site'].isin(site_list)]\n",
    "            prob_escape_final['site'] = pd.Categorical(prob_escape_final['site'], ordered=True)\n",
    "            # prob_escape_final['site'] = prob_escape_final['site'].astype(str)\n",
    "\n",
    "        else:\n",
    "            prob_escape_final = prob_escape_sum.copy()\n",
    "            \n",
    "        prob_escape_final['serum'] = serum\n",
    "        prob_escape_final['cohort'] = age_group\n",
    "        \n",
    "        summed_escape_list.append(prob_escape_final)\n",
    "        \n",
    "    summed_escape = pd.concat(summed_escape_list)\n",
    "    return summed_escape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57b77088-df77-414e-8c10-483e5b195210",
   "metadata": {},
   "outputs": [],
   "source": [
    "peds = [3944, 2389, 2323, 2388, 3973, 4299, 4584, 2367]\n",
    "teens = [2350, 2365, 2382, 3866, 2380, 3856, 3857, 3862]\n",
    "adults = ['33C', '34C', '197C', '199C', '215C', \n",
    "          '210C', '74C', '68C', '150C', '18C',]\n",
    "\n",
    "sample_lists = [peds, teens, adults]\n",
    "cohorts = ['2-5_years', '15-20_years', '40-45_years']\n",
    "\n",
    "summed_escapes = []\n",
    "i=0 # for looping through age cohort definitions\n",
    "\n",
    "for entry in sample_lists:\n",
    "    summed_escape = get_summed_escapes(entry, cohorts[i])\n",
    "    summed_escapes.append(summed_escape)\n",
    "\n",
    "    i+=1\n",
    "\n",
    "escape_df_hk19 = pd.concat(summed_escapes)\n",
    "\n",
    "escape_df_hk19['serum'] = escape_df_hk19['serum'].astype(str)\n",
    "escape_df_hk19['ha_strain'] = 'hk19'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b028a0cf-6839-46c1-8c00-9b155baa54fd",
   "metadata": {},
   "source": [
    "### Get Perth09 escape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9611eb7-3b20-4255-b7a8-2f27d8da470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define samples in each age cohort\n",
    "sample_dict = {\n",
    "    \"2-4_years\": [\n",
    "        \"age 2.1 (Vietnam)\", \n",
    "        \"age 2.2 (Vietnam)\",\n",
    "        \"age 2.4 (Vietnam)\",\n",
    "        \"age 2.5 (Vietnam)\",\n",
    "        \"age 2.5b (Vietnam)\",\n",
    "        \"age 3.3 (Vietnam)\", \n",
    "        \"age 3.3b (Vietnam)\",\n",
    "        \"age 3.4 (Vietnam)\", \n",
    "        \"age 3.5 (Vietnam)\",\n",
    "    ],   \n",
    "    \"30-34_years\": [\n",
    "        \"age 30.5 (Vietnam)\",\n",
    "        \"age 31.5 (Vietnam)\",\n",
    "        \"age 33.5 (Vietnam)\",\n",
    "    ],\n",
    "    \"misc_adult\": [\n",
    "        \"age 21 (Seattle)\",\n",
    "        \"age 53 (Seattle)\",\n",
    "        \"age 64 (Seattle)\",\n",
    "        \"age 65 (Seattle)\",\n",
    "    ],\n",
    "    \"ferret\": [\n",
    "        \"ferret 1 (Pitt)\",\n",
    "        \"ferret 2 (Pitt)\",\n",
    "        \"ferret 3 (Pitt)\",\n",
    "        \"ferret (WHO)\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "# get full dataset\n",
    "escape_df = pd.read_csv(f'results/perth2009/merged_escape.csv')[['name', 'site', 'wildtype', 'mutant', 'escape']]\n",
    "escape_df = escape_df.rename(columns={'name': 'serum'})\n",
    "\n",
    "# Function to convert '(HA2)X' to numeric\n",
    "def convert_site_to_numeric(site):\n",
    "    if '(HA2)' in site:\n",
    "        try:\n",
    "            number = int(site.replace('(HA2)', '').strip())\n",
    "            return number + 329\n",
    "        except ValueError:\n",
    "            return site  # If there's an issue with conversion, return the original value\n",
    "    else:\n",
    "        return site\n",
    "\n",
    "# Apply the function to the 'site' column\n",
    "escape_df['site'] = escape_df['site'].apply(convert_site_to_numeric)\n",
    "\n",
    "# get summed escape at each site\n",
    "escape_df = escape_df.groupby(['serum', 'site', 'wildtype'], as_index=False).aggregate({'escape': 'sum'})\n",
    "\n",
    "# floor at 0\n",
    "escape_df['escape'] = escape_df['escape'].clip(lower=0)\n",
    "\n",
    "# add cohort label\n",
    "def find_sample_type(sample_name):\n",
    "    for sample_type, sample_list in sample_dict.items():\n",
    "        if sample_name in sample_list:\n",
    "            return sample_type\n",
    "    return None\n",
    "\n",
    "escape_df['cohort'] = escape_df['serum'].apply(find_sample_type)\n",
    "\n",
    "escape_df = escape_df.loc[(escape_df['cohort'] != 'misc_adult') & (escape_df['cohort'] != 'ferret')]\n",
    "escape_df['site'] = escape_df['site'].astype(int)\n",
    "\n",
    "escape_df['ha_strain'] = 'perth09'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb7f4cf-eb3c-48d2-a9d8-e833317a1f5c",
   "metadata": {},
   "source": [
    "### Merge datasets and calculate max normalized escape value for each cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af9a9029-adf5-4f7f-9748-fda4c030e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "escape_df_full = pd.concat([escape_df, escape_df_hk19])\n",
    "\n",
    "# Group the DataFrame by the 'serum' column\n",
    "grouped = escape_df_full.groupby('serum')\n",
    "\n",
    "# Define a function to normalize the 'escape_mean' column within each group\n",
    "def normalize(group):\n",
    "    group['escape'] = group['escape'] / group['escape'].max()\n",
    "    return group\n",
    "\n",
    "# Apply the normalization function to each group\n",
    "normalized_df = grouped.apply(normalize)\n",
    "\n",
    "# Reset the index of the resulting DataFrame\n",
    "normalized_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Clip to just the positive values\n",
    "normalized_df['escape'] = normalized_df['escape'].clip(lower=0)\n",
    "\n",
    "# add mean cohort escape column\n",
    "normalized_df['mean_cohort_escape'] = (\n",
    "    normalized_df.groupby(['site', 'cohort'])['escape']\n",
    "    .transform('mean')\n",
    ")\n",
    "\n",
    "# add max cohort escape column\n",
    "normalized_df['max_cohort_escape'] = (\n",
    "    normalized_df.groupby(['site', 'cohort'])['escape']\n",
    "    .transform('max')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "364bfd91-8429-4570-98a0-024b6cc94a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame by grouping 'normalized_df' by the 'cohort' column\n",
    "grouped = normalized_df.groupby('cohort')\n",
    "\n",
    "# Initialize an empty list to store DataFrames for each cohort\n",
    "cohort_dfs = []\n",
    "\n",
    "# Loop through each cohort group\n",
    "for cohort, cohort_data in grouped:\n",
    "    # Create a DataFrame containing unique sites and their mean_cohort_escape values\n",
    "    cohort_escape = cohort_data[['site', 'mean_cohort_escape', 'max_cohort_escape', 'ha_strain', 'cohort']].drop_duplicates()\n",
    "\n",
    "    # Add chain identifiers, which will duplicate sites 3x\n",
    "    chain_dfs = []\n",
    "    chains = ['A', 'C', 'E']\n",
    "    for chain in chains:\n",
    "        chain_df = cohort_escape.copy()\n",
    "        chain_df['chain'] = chain\n",
    "        # cohort_escape['chain'] = chain\n",
    "        chain_dfs.append(chain_df)\n",
    "\n",
    "    cohort_escape = pd.concat(chain_dfs, ignore_index=True)\n",
    "    \n",
    "    # Append this cohort's data to the list\n",
    "    cohort_dfs.append(cohort_escape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2d7a8b-f33f-425d-8404-4e025dc623a8",
   "metadata": {},
   "source": [
    "### Reassign B-factors in PDB file for each cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "904ce066-5878-49ad-ab9e-075e9675eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reassign_b_factor(\n",
    "    input_pdbfile,\n",
    "    output_pdbfile,\n",
    "    df,\n",
    "    metric_col,\n",
    "    *,\n",
    "    site_col=\"site\",\n",
    "    chain_col=\"chain\",\n",
    "    missing_metric=0,\n",
    "    model_index=0,\n",
    "):\n",
    "\n",
    "    # subset `df` to needed columns and error check it\n",
    "    cols = [metric_col, site_col, chain_col]\n",
    "    for col in cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"`df` lacks column {col}\")\n",
    "    df = df[cols].drop_duplicates()\n",
    "    if len(df) != len(df.groupby([site_col, chain_col])):\n",
    "        raise ValueError(\"non-unique metric for a site in a chain\")\n",
    "\n",
    "    # read PDB, catch warnings about discontinuous chains\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\n",
    "            \"ignore\", category=Bio.PDB.PDBExceptions.PDBConstructionWarning\n",
    "        )\n",
    "        pdb = Bio.PDB.PDBParser().get_structure(\"_\", input_pdbfile)\n",
    "\n",
    "    # get the model out of the PDB\n",
    "    model = list(pdb.get_models())[model_index]\n",
    "\n",
    "    # make sure all chains in PDB\n",
    "    missing_chains = set(df[chain_col]) - {chain.id for chain in model.get_chains()}\n",
    "    if missing_chains:\n",
    "        raise ValueError(f\"`df` has chains not in PDB: {missing_chains}\")\n",
    "\n",
    "    # make missing_metric a dict if it isn't already\n",
    "    if not isinstance(missing_metric, dict):\n",
    "        missing_metric = {chain.id: missing_metric for chain in model.get_chains()}\n",
    "\n",
    "    # loop over all chains and do coloring\n",
    "    for chain in model.get_chains():\n",
    "        chain_id = chain.id\n",
    "        site_to_val = (\n",
    "            df.query(f\"{chain_col} == @chain_id\")\n",
    "            .set_index(site_col)[metric_col]\n",
    "            .to_dict()\n",
    "        )\n",
    "        for residue in chain:\n",
    "            site = residue.get_id()[1]\n",
    "            try:\n",
    "                metric_val = site_to_val[site]\n",
    "            except KeyError:\n",
    "                metric_val = missing_metric[chain_id]\n",
    "            # for disordered residues, get list of them\n",
    "            try:\n",
    "                residuelist = residue.disordered_get_list()\n",
    "            except AttributeError:\n",
    "                residuelist = [residue]\n",
    "            for r in residuelist:\n",
    "                for atom in r:\n",
    "                    # for disordered atoms, get list of them\n",
    "                    try:\n",
    "                        atomlist = atom.disordered_get_list()\n",
    "                    except AttributeError:\n",
    "                        atomlist = [atom]\n",
    "                    for a in atomlist:\n",
    "                        a.bfactor = metric_val\n",
    "\n",
    "    # write PDB\n",
    "    io = Bio.PDB.PDBIO()\n",
    "    io.set_structure(pdb)\n",
    "    io.save(output_pdbfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "403365bb-f94b-41f8-aa4c-f0c0382d24e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in cohort_dfs:\n",
    "    cohort_name = df['cohort'].iloc[0]\n",
    "    ha_strain = df['ha_strain'].iloc[0]\n",
    "    reassign_b_factor(input_pdbfile='data/PDBs/4o5n.pdb',\n",
    "                      output_pdbfile=f'figures/structure_plots/escape_score_pdbs/{ha_strain}_{cohort_name}_normalized_max.pdb',\n",
    "                      df=df,\n",
    "                      metric_col='max_cohort_escape',\n",
    "                      site_col=\"site\",\n",
    "                      chain_col=\"chain\",\n",
    "                      missing_metric=0,\n",
    "                      model_index=0,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
